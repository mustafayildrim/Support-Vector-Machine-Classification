{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EU9nI8EWbrS"
   },
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "## By: Mustafa Yildirim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jw48ZfUexHP"
   },
   "source": [
    "Implementing binary classification using Support Vector Machines (SVMs) to distinguish between digits '7' and '9'. \n",
    "\n",
    "Training both a linear SVM and a nonlinear SVM with a Gaussian RBF kernel, while fine-tuning hyperparameters (C and γ) for optimal performance. \n",
    "\n",
    "Evaluating the classification accuracy on held-out test data and comparing the Scikit-learn implementation with the Projected Gradient Descent (PGD) optimizer in terms of accuracy and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtMQYNm2Klnj",
    "outputId": "6f68d26f-afa1-4b89-9d2a-46aa0a756914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306,
     "referenced_widgets": [
      "53ff35d2162b40e6bbefc446fcad88d1",
      "988bcaef27fb4ef68dabc2c706c62711",
      "ff0abd976db14608ae914f0cdbc27072",
      "a13ef9e210234190b12d21cc95f8ac5d",
      "0508112963ea4abd807d983148dddd10",
      "59467ac1fd54480c99a416f9764b6261",
      "7239552d64f94788a20cda9bc41388f0",
      "bc717e2d12db444eb6bb8de75c37f83f",
      "32717635dd014854bb4d2a33bb5ece4e",
      "cdd7c0efd3c64b9da6a381dddbb10436",
      "25d2fc2018af42d088c9c18be692d1bd",
      "c208f26ee68846a5b709724865504e82",
      "3b9507edfd5a449fad6c7505fd36177e",
      "bbd402246ea24b23bb9f41568eea42db",
      "84a7c5012c90485d9ba707643a82ed68",
      "31178300686849eb99774088dec45bc5",
      "15712eb233924615acc73174d5429bff",
      "38c5b6155fe948729be821a1a6c1e71d",
      "1b90597767a941808eb00f7764667e78",
      "98c78c9b01404342bdf7710b623f9212",
      "281de5eae7d24e9b9e011c628789e022",
      "1462babca23f4bdfb5f0a5b6aaa2a899",
      "88bbf5a363094a14bc24e9cce201f850",
      "2e7fe4ebfbd5422489d53595c48ec56c",
      "ecb5b5813add4a70b9c8dfd38441e7ce",
      "714c9b185a794f228748c5469d4a22df",
      "8c046eb690dd40cba5a00c5f5098fef0",
      "5059e7b182f843af8b765da374d83be2",
      "e686ee57bc484f5c8c98f7f061074c6e",
      "7fa8eaead0f84e608a644c638b940f1c",
      "5218c659af1c45149025292ce4167482",
      "4e479ba250c048e4b3e2dc15ceaf8f62",
      "a8b95d1fe21348dd919bc5c86288c042",
      "32b1ef670ed54dd7b15ee256d19f747e",
      "56579a472ee2442682f9791938e10fca",
      "75ba5cfcc1804175b9b099ecba468a73",
      "bc5efb42a3874067abbb6c9a61516dcb",
      "1fc49472167345b1abf48c3c22f282f1",
      "e3585bd7187a4dc09ccfaf216f66e3cb",
      "0a9c1ebf71a448e3bff930e297d0f9ef",
      "a42b9fb2fcf8434bab4780d85c6cdb14",
      "7c0397842b8b4cd9a55222976cadba71",
      "2f90007ac3cd4dbb83de80230155847a",
      "94ec554b6d7b4a17b48bd17ca6d84f74",
      "a04ddc7e1cdd443098cb5dd1c69b8023",
      "d122c626e86f4631ae6e6d1595595ccf",
      "cfcbe7d178104b599fe30c62817f8744",
      "b521b873ff42419ab50d2dcc1cc516a1",
      "f793978fc38d4ac1926b27a316621db2",
      "b240308b881c43209d5dc6c9db40d4be",
      "88f2d20b9b614a508d4e31b20edc1a06",
      "981b93e6301f4e5891b06c99ca98f3a4",
      "ad5343299c1242fcbbb3a5cfc440a8ff",
      "a7505bac0b54484d89baa182668545fb",
      "66f3e7caf1ed411ea14839dd892b901d"
     ]
    },
    "id": "TdsbHwhBJ7rp",
    "outputId": "993f7499-3b79-4f69-879d-9bdca48e3de5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ff35d2162b40e6bbefc446fcad88d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c208f26ee68846a5b709724865504e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/15.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bbf5a363094a14bc24e9cce201f850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/2.60M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b1ef670ed54dd7b15ee256d19f747e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04ddc7e1cdd443098cb5dd1c69b8023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# load MNIST data using Datasets at Hugging Face\n",
    "#\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "trainset = load_dataset('mnist', split='train')\n",
    "train_data = trainset['image']\n",
    "train_label = trainset['label']\n",
    "\n",
    "testset = load_dataset('mnist', split='test')\n",
    "test_data = testset['image']\n",
    "test_label = testset['label']\n",
    "\n",
    "train_data = np.array(train_data, dtype='float')/255 # norm to [0,1]\n",
    "train_data = np.reshape(train_data,(60000,28*28))\n",
    "train_label = np.array(train_label, dtype='short')\n",
    "test_data = np.array(test_data, dtype='float')/255 # norm to [0,1]\n",
    "test_data = np.reshape(test_data,(10000,28*28))\n",
    "test_label = np.array(test_label, dtype='short')\n",
    "\n",
    "print(train_data.shape, train_label.shape, test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ITdJXF0LBIK"
   },
   "outputs": [],
   "source": [
    "# prepare digits '7' and '9' for binary SVMs\n",
    "digit_train_index = np.logical_or(train_label == 7, train_label == 9)\n",
    "X_train = train_data[digit_train_index]\n",
    "y_train = train_label[digit_train_index]\n",
    "digit_test_index = np.logical_or(test_label == 7, test_label == 9)\n",
    "X_test = test_data[digit_test_index]\n",
    "y_test = test_label[digit_test_index]\n",
    "\n",
    "# normalize all feature vectors to unit-length\n",
    "X_train = np.transpose (X_train.T / np.sqrt(np.sum(X_train*X_train, axis=1)))\n",
    "X_test =  np.transpose (X_test.T  / np.sqrt(np.sum(X_test*X_test, axis=1)))\n",
    "\n",
    "# convert labels: '7' => -1, '9' => +1\n",
    "CUTOFF = 8 # any number between '7' and '9'\n",
    "y_train = np.sign(y_train-CUTOFF)\n",
    "y_test = np.sign(y_test-CUTOFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dP4fjScvN-NU"
   },
   "source": [
    "Sklearn Linear SVM\n",
    "\n",
    "Displaying the accuracy with different C values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnLuINm-N9pF",
    "outputId": "5e6c0afb-fbab-44a3-a23c-605be23412fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear SVM (C=0.0001): training accuracy=51.29%  test accuracy=50.47%\n",
      "linear SVM (C=0.001): training accuracy=74.64%  test accuracy=77.86%\n",
      "linear SVM (C=0.01): training accuracy=93.95%  test accuracy=94.65%\n",
      "linear SVM (C=0.1): training accuracy=95.60%  test accuracy=96.12%\n",
      "linear SVM (C=1): training accuracy=96.39%  test accuracy=96.51%\n",
      "linear SVM (C=2): training accuracy=96.72%  test accuracy=96.71%\n",
      "linear SVM (C=4): training accuracy=96.95%  test accuracy=96.86%\n",
      "linear SVM (C=10): training accuracy=97.05%  test accuracy=97.01%\n",
      "linear SVM (C=20): training accuracy=97.23%  test accuracy=97.01%\n",
      "linear SVM (C=50): training accuracy=97.45%  test accuracy=96.86%\n",
      "linear SVM (C=100): training accuracy=97.54%  test accuracy=96.76%\n",
      "linear SVM (C=500): training accuracy=97.74%  test accuracy=96.51%\n",
      "linear SVM (C=1000): training accuracy=97.85%  test accuracy=96.42%\n"
     ]
    }
   ],
   "source": [
    "# linear SVM: use sk-learn SVC functions\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "for c in [0.0001, 0.001, 0.01, 0.1, 1, 2, 4, 10, 20, 50, 100, 500, 1000]:\n",
    "  linearSVM = SVC(kernel='linear', C=c)\n",
    "  linearSVM.fit(X_train,y_train)\n",
    "  predict = linearSVM.predict(X_train)\n",
    "  train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
    "  predict = linearSVM.predict(X_test)\n",
    "  test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
    "  print(f'linear SVM (C={c}): training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVTRMJjuOCvd"
   },
   "source": [
    "Sklearn Nonlinear SVM with Gaussian RBF kernel\n",
    "\n",
    "Displaying the accuracy with different C and gamma values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_S_iMTmLXfLV",
    "outputId": "f12b8ca0-9340-445d-c31e-ab2207487eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonlinear RBF SVM (C=0.01,gamma=scale): training accuracy=95.48%  test accuracy=95.78%\n",
      "nonlinear RBF SVM (C=0.01,gamma=0.001): training accuracy=51.29%  test accuracy=50.47%\n",
      "nonlinear RBF SVM (C=0.01,gamma=0.01): training accuracy=51.29%  test accuracy=50.47%\n",
      "nonlinear RBF SVM (C=0.01,gamma=0.1): training accuracy=92.41%  test accuracy=93.08%\n",
      "nonlinear RBF SVM (C=0.01,gamma=1): training accuracy=95.31%  test accuracy=95.63%\n",
      "nonlinear RBF SVM (C=0.01,gamma=10): training accuracy=51.29%  test accuracy=50.47%\n",
      "nonlinear RBF SVM (C=0.01,gamma=100): training accuracy=51.29%  test accuracy=50.47%\n",
      "nonlinear RBF SVM (C=0.1,gamma=scale): training accuracy=97.87%  test accuracy=97.74%\n",
      "nonlinear RBF SVM (C=0.1,gamma=0.001): training accuracy=51.29%  test accuracy=50.47%\n",
      "nonlinear RBF SVM (C=0.1,gamma=0.01): training accuracy=92.41%  test accuracy=93.27%\n",
      "nonlinear RBF SVM (C=0.1,gamma=0.1): training accuracy=94.63%  test accuracy=95.19%\n",
      "nonlinear RBF SVM (C=0.1,gamma=1): training accuracy=97.67%  test accuracy=97.55%\n",
      "nonlinear RBF SVM (C=0.1,gamma=10): training accuracy=99.53%  test accuracy=96.47%\n",
      "nonlinear RBF SVM (C=0.1,gamma=100): training accuracy=51.29%  test accuracy=50.47%\n",
      "nonlinear RBF SVM (C=1,gamma=scale): training accuracy=99.62%  test accuracy=98.82%\n",
      "nonlinear RBF SVM (C=1,gamma=0.001): training accuracy=92.41%  test accuracy=93.27%\n",
      "nonlinear RBF SVM (C=1,gamma=0.01): training accuracy=94.48%  test accuracy=94.99%\n",
      "nonlinear RBF SVM (C=1,gamma=0.1): training accuracy=96.36%  test accuracy=96.76%\n",
      "nonlinear RBF SVM (C=1,gamma=1): training accuracy=99.53%  test accuracy=98.77%\n",
      "nonlinear RBF SVM (C=1,gamma=10): training accuracy=100.00%  test accuracy=98.97%\n",
      "nonlinear RBF SVM (C=1,gamma=100): training accuracy=100.00%  test accuracy=50.47%\n",
      "nonlinear RBF SVM (C=10,gamma=scale): training accuracy=100.00%  test accuracy=99.36%\n",
      "nonlinear RBF SVM (C=10,gamma=0.001): training accuracy=94.48%  test accuracy=95.04%\n",
      "nonlinear RBF SVM (C=10,gamma=0.01): training accuracy=95.91%  test accuracy=96.47%\n",
      "nonlinear RBF SVM (C=10,gamma=0.1): training accuracy=98.31%  test accuracy=97.79%\n",
      "nonlinear RBF SVM (C=10,gamma=1): training accuracy=99.99%  test accuracy=99.31%\n",
      "nonlinear RBF SVM (C=10,gamma=10): training accuracy=100.00%  test accuracy=98.97%\n",
      "nonlinear RBF SVM (C=10,gamma=100): training accuracy=100.00%  test accuracy=50.47%\n",
      "nonlinear RBF SVM (C=100,gamma=scale): training accuracy=100.00%  test accuracy=99.31%\n",
      "nonlinear RBF SVM (C=100,gamma=0.001): training accuracy=95.88%  test accuracy=96.32%\n",
      "nonlinear RBF SVM (C=100,gamma=0.01): training accuracy=96.91%  test accuracy=96.91%\n",
      "nonlinear RBF SVM (C=100,gamma=0.1): training accuracy=99.68%  test accuracy=98.67%\n",
      "nonlinear RBF SVM (C=100,gamma=1): training accuracy=100.00%  test accuracy=99.31%\n",
      "nonlinear RBF SVM (C=100,gamma=10): training accuracy=100.00%  test accuracy=98.97%\n",
      "nonlinear RBF SVM (C=100,gamma=100): training accuracy=100.00%  test accuracy=50.47%\n",
      "nonlinear RBF SVM (C=1000,gamma=scale): training accuracy=100.00%  test accuracy=99.31%\n",
      "nonlinear RBF SVM (C=1000,gamma=0.001): training accuracy=96.73%  test accuracy=96.71%\n",
      "nonlinear RBF SVM (C=1000,gamma=0.01): training accuracy=98.21%  test accuracy=97.55%\n",
      "nonlinear RBF SVM (C=1000,gamma=0.1): training accuracy=100.00%  test accuracy=98.92%\n",
      "nonlinear RBF SVM (C=1000,gamma=1): training accuracy=100.00%  test accuracy=99.31%\n",
      "nonlinear RBF SVM (C=1000,gamma=10): training accuracy=100.00%  test accuracy=98.97%\n",
      "nonlinear RBF SVM (C=1000,gamma=100): training accuracy=100.00%  test accuracy=50.47%\n"
     ]
    }
   ],
   "source": [
    "# nonlinear SVM (Gaussian RBF kernel): use sk-learn SVC functions\n",
    "for c in [0.01, 0.1, 1, 10, 100, 1000]:\n",
    "  for g in ['scale', 0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    rbfSVM = SVC(kernel='rbf', C=c, gamma=g)\n",
    "    rbfSVM.fit(X_train,y_train)\n",
    "    predict = rbfSVM.predict(X_train)\n",
    "    train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
    "    predict = rbfSVM.predict(X_test)\n",
    "    test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
    "    print(f'nonlinear RBF SVM (C={c},gamma={g}): training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8A5-PsiQXh7"
   },
   "source": [
    "PGD Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnCx435KQXpU"
   },
   "outputs": [],
   "source": [
    "# solve linear SVMs using projected gradient descent (PGD)\n",
    "\n",
    "class mySVM1():\n",
    "  def __init__(self, kernel='linear', optimizer='pgd', debug=0, threshold=0.001, \\\n",
    "               lr=1.0, max_epochs=10, batch_size=2, C=1):\n",
    "    self.kernel = kernel        # kernel type\n",
    "    self.optimizer = optimizer  # which optimizer is used to solve quadratic programming\n",
    "    self.lr = lr                # max learning rate in PGD\n",
    "    self.max_epochs = max_epochs   # max epochs in PGD\n",
    "    self.batch_size = batch_size   # size of each subset in PGD\n",
    "    self.debug = debug             # whether print debugging info\n",
    "    self.threshold = threshold     # threshold to filter out support vectors\n",
    "\n",
    "    self.C = C     # C for the soft-margin term\n",
    "\n",
    "  # Linear Kernel Function\n",
    "  # X[N,d]: training samples;  Y[M,d]: other training samples\n",
    "  # return Q[N,N]: linear kernel matrix between X and Y\n",
    "  def Kernel(self, X, Y):\n",
    "    if (self.kernel == 'linear'):\n",
    "      K = X @ Y.T\n",
    "\n",
    "    return K\n",
    "\n",
    "  # construct matrix Q from any kernel function for dual SVM optimization\n",
    "  def QuadraticMatrix(self, X, y):\n",
    "    Q = np.outer(y, y) * self.Kernel(X, X)\n",
    "    return Q\n",
    "\n",
    "  # use projected gradient descent to solve quadratic program\n",
    "  # refer to Algorithm 6.5 on page 127\n",
    "  # Q[N,N]: quadratic matrix;  y[N]: training labels (+1 or -1)\n",
    "  def PGD(self, Q, y):\n",
    "    N = Q.shape[0]   # num of training samples\n",
    "    alpha = np.zeros(N)\n",
    "    prev_L = 0.0\n",
    "\n",
    "    for epoch in range(self.max_epochs):\n",
    "      indices = np.random.permutation(N)  #randomly shuffle data indices\n",
    "      for batch_start in range(0, N, self.batch_size):\n",
    "        idx = indices[batch_start:batch_start + self.batch_size] # indices of the selected subset\n",
    "        alpha_s = alpha[idx]\n",
    "        y_s = y[idx]\n",
    "\n",
    "        grad_s = Q[idx,:] @ alpha - np.ones(idx.shape[0])\n",
    "        proj_grad_s = grad_s - np.dot(y_s,grad_s)/np.dot(y_s, y_s)*y_s\n",
    "\n",
    "        bound = np.zeros(idx.shape[0])\n",
    "        bound[proj_grad_s < 0] = self.C\n",
    "\n",
    "        eta = np.min(np.abs(alpha_s-bound)/(np.abs(proj_grad_s)+0.001))\n",
    "\n",
    "        alpha[idx] -= min(eta, self.lr) * proj_grad_s\n",
    "\n",
    "      L = 0.5 * alpha.T @ Q @ alpha - np.sum(alpha) # objectibve function\n",
    "      if (L > prev_L):\n",
    "        if (self.debug>0):\n",
    "          print('Early stopping at epoch={epoch}!')\n",
    "        break\n",
    "\n",
    "      if (self.debug>1):\n",
    "        print(f'[PGD optimizer] epoch = {epoch}: L = {L:.5f}  (# of support vectors = {(alpha>self.threshold).sum()})')\n",
    "        print(f'                 alpha: max={np.max(alpha)} min={np.min(alpha)} orthogonal constraint={np.dot(alpha,y):.2f}')\n",
    "\n",
    "      prev_L = L\n",
    "\n",
    "    return alpha\n",
    "\n",
    "  # train SVM from training samples\n",
    "  # X[N,d]: input features;  y[N]: output labels (+1 or -1)\n",
    "  def fit(self, X, y):\n",
    "    if(self.kernel != 'linear'):\n",
    "      print(\"Error: only linear kernel is supported!\")\n",
    "      return\n",
    "\n",
    "    Q = self.QuadraticMatrix(X, y)\n",
    "\n",
    "    alpha = self.PGD(Q, y)\n",
    "\n",
    "    #save support vectors (pruning all data with alpha==0)\n",
    "    self.X_SVs = X[alpha>self.threshold]\n",
    "    self.y_SVs = y[alpha>self.threshold]\n",
    "    self.alpha_SVs = alpha[alpha>self.threshold]\n",
    "\n",
    "    # compute weight vector for linear SVMs (refer to the formula on page 120)\n",
    "    if(self.kernel == 'linear'):\n",
    "      self.w = (self.y_SVs * self.alpha_SVs) @ self.X_SVs\n",
    "\n",
    "    # estimate b\n",
    "    idx = np.nonzero(np.logical_and(self.alpha_SVs>self.threshold,self.alpha_SVs<self.C-self.threshold))\n",
    "    if(len(idx) == 0):\n",
    "      idx = np.nonzero(self.alpha_SVs>self.threshold)\n",
    "    # refer to the formula on page 125 (above Figure 6.11)\n",
    "    b = self.y_SVs[idx] - (self.y_SVs * self.alpha_SVs) @ self.Kernel(self.X_SVs, self.X_SVs[idx])\n",
    "    self.b = np.median(b)\n",
    "\n",
    "    return\n",
    "\n",
    "  # use SVM from prediction\n",
    "  # X[N,d]: input features\n",
    "  def predict(self, X):\n",
    "    if(self.kernel != 'linear'):\n",
    "      print(\"Error: only linear kernel is supported!\")\n",
    "      return\n",
    "\n",
    "    y = X @ self.w + self.b\n",
    "\n",
    "    return np.sign(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cuLzokct53-"
   },
   "source": [
    "Displaying the accuracy with different C values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3jD30BjTV_t",
    "outputId": "40ba2ec7-583d-48d8-af80-91a6f3518aa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY linear SVM (C=0.1): training accuracy=95.40%  test accuracy=96.17%\n",
      "MY linear SVM (C=1): training accuracy=95.85%  test accuracy=96.27%\n",
      "MY linear SVM (C=2): training accuracy=96.32%  test accuracy=96.71%\n",
      "MY linear SVM (C=4): training accuracy=96.26%  test accuracy=96.17%\n",
      "MY linear SVM (C=10): training accuracy=95.97%  test accuracy=96.22%\n"
     ]
    }
   ],
   "source": [
    "for c in [0.1, 1, 2, 4, 10]:\n",
    "  svm = mySVM1(max_epochs=10, lr=2.0, C=c, kernel='linear')\n",
    "  svm.fit(X_train,y_train)\n",
    "\n",
    "  predict = svm.predict(X_train)\n",
    "  train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
    "  predict = svm.predict(X_test)\n",
    "  test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
    "  print(f'MY linear SVM (C={c}): training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfoPeo_i8Lx8"
   },
   "source": [
    "Displaying the timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJ5WhAjtTa6f",
    "outputId": "47233fa0-14e0-40eb-d5fb-324cf4cf7714"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.4 s ± 134 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "11.6 s ± 407 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "c=1\n",
    "\n",
    "# Scikit-learn Linear SVM\n",
    "linearSVM = SVC(kernel='linear', C=c)\n",
    "%timeit linearSVM.fit(X_train,y_train)\n",
    "\n",
    "# PGD-based linear SVM\n",
    "svm = mySVM1(max_epochs=10, lr=2.0, C=c, kernel='linear')\n",
    "%timeit svm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3dhBkk-QXwR"
   },
   "source": [
    "PGD Nonlinear SVM with Gaussian RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYUACvwoQX30"
   },
   "outputs": [],
   "source": [
    "# extend for nonlinear SVMs by adding polynomial and RBF kernel functions\n",
    "#\n",
    "\n",
    "class mySVM2():\n",
    "  def __init__(self, kernel='linear', optimizer='pgd', debug=0, threshold=0.001, \\\n",
    "               lr=1.0, max_epochs=20, batch_size=2, C=1, order=3, gamma=1.0):\n",
    "    self.kernel = kernel           # kernel type\n",
    "    self.optimizer = optimizer     # which optimizer is used to solve quadratic programming\n",
    "    self.lr = lr                   # max learning rate in PGD\n",
    "    self.max_epochs = max_epochs   # max epochs in PGD\n",
    "    self.batch_size = batch_size   # size of each subset in PGD\n",
    "    self.debug = debug             # whether print debugging info\n",
    "    self.threshold = threshold     # threshold to filter out support vectors\n",
    "\n",
    "    self.C = C                     # C for the soft-margin term\n",
    "    self.order = order             # power order for polynomial kernel\n",
    "    self.gamma = gamma             # gamma for Gaussian RBF kernel\n",
    "\n",
    "  # Kernel Function\n",
    "  # X[N,d]: training samples;  Y[M,d]: other training samples\n",
    "  # return Q[N,N]: linear kernel matrix between X and Y\n",
    "  def Kernel(self, X, Y):\n",
    "    if (self.kernel == 'linear'):\n",
    "      K = X @ Y.T\n",
    "    elif (self.kernel == 'poly'):\n",
    "      K = np.power(X @ Y.T +1, self.order)\n",
    "    elif (self.kernel == 'rbf'):\n",
    "      d1 = np.sum(X*X, axis=1)\n",
    "      d2 = np.sum(Y*Y, axis=1)\n",
    "      K = np.outer(d1, np.ones(Y.shape[0])) + np.outer(np.ones(X.shape[0]), d2) \\\n",
    "          - 2 * X @ Y.T\n",
    "      K = np.exp(-self.gamma * K)\n",
    "\n",
    "    return K\n",
    "\n",
    "  # construct matrix Q from any kernel function for dual SVM optimization\n",
    "  def QuadraticMatrix(self, X, y):\n",
    "    Q = np.outer(y, y) * self.Kernel(X, X)\n",
    "    return Q\n",
    "\n",
    "  # use projected gradient descent to solve quadratic program\n",
    "  # refer to Algorithm 6.5 on page 127\n",
    "  # Q[N,N]: quadratic matrix;  y[N]: training labels (+1 or -1)\n",
    "  def PGD(self, Q, y):\n",
    "    N = Q.shape[0]   # num of training samples\n",
    "    alpha = np.zeros(N)\n",
    "    prev_L = 0.0\n",
    "\n",
    "    for epoch in range(self.max_epochs):\n",
    "      indices = np.random.permutation(N)  #randomly shuffle data indices\n",
    "      for batch_start in range(0, N, self.batch_size):\n",
    "        idx = indices[batch_start:batch_start + self.batch_size] # indices of the current subset\n",
    "        alpha_s = alpha[idx]\n",
    "        y_s = y[idx]\n",
    "\n",
    "        grad_s = Q[idx,:] @ alpha - np.ones(idx.shape[0])\n",
    "        proj_grad_s = grad_s - np.dot(y_s,grad_s)/np.dot(y_s, y_s)*y_s\n",
    "\n",
    "        bound = np.zeros(idx.shape[0])\n",
    "        bound[proj_grad_s < 0] = self.C\n",
    "\n",
    "        eta = np.min(np.abs(alpha_s-bound)/(np.abs(proj_grad_s)+0.001))\n",
    "\n",
    "        alpha[idx] -= min(eta, self.lr) * proj_grad_s\n",
    "\n",
    "      L = 0.5 * alpha.T @ Q @ alpha - np.sum(alpha) # objectibve function\n",
    "      if (L > prev_L):\n",
    "        if (self.debug>0):\n",
    "          print(f'Early stopping at epoch={epoch}! (reduce learning rate lr)')\n",
    "        break\n",
    "\n",
    "      if (self.debug>1):\n",
    "        print(f'[PGD optimizer] epoch = {epoch}: L = {L:.5f}  (# of support vectors = {(alpha>self.threshold).sum()})')\n",
    "        print(f'                 alpha: max={np.max(alpha)} min={np.min(alpha)} orthogonal constraint={np.dot(alpha,y):.2f}')\n",
    "\n",
    "      prev_L = L\n",
    "\n",
    "    return alpha\n",
    "\n",
    "  # train SVM from training samples\n",
    "  # X[N,d]: input features;  y[N]: output labels (+1 or -1)\n",
    "  def fit(self, X, y):\n",
    "    if(self.kernel != 'linear' and self.kernel != 'poly' and self.kernel != 'rbf'):\n",
    "      print(\"Error: only linear/poly/rbf kernel is supported!\")\n",
    "      return\n",
    "\n",
    "    Q = self.QuadraticMatrix(X, y)\n",
    "\n",
    "    alpha = self.PGD(Q, y)\n",
    "\n",
    "    #save support vectors (pruning all data with alpha==0)\n",
    "    self.X_SVs = X[alpha>self.threshold]\n",
    "    self.y_SVs = y[alpha>self.threshold]\n",
    "    self.alpha_SVs = alpha[alpha>self.threshold]\n",
    "\n",
    "    if(self.kernel == 'linear'):\n",
    "      self.w = (self.y_SVs * self.alpha_SVs) @ self.X_SVs\n",
    "\n",
    "    # estimate b\n",
    "    idx = np.nonzero(np.logical_and(self.alpha_SVs>self.threshold,self.alpha_SVs<self.C-self.threshold))\n",
    "    if(len(idx) == 0):\n",
    "      idx = np.nonzero(self.alpha_SVs>self.threshold)\n",
    "    # refer to the formula on page 125 (above Figure 6.11)\n",
    "    b = self.y_SVs[idx] - (self.y_SVs * self.alpha_SVs) @ self.Kernel(self.X_SVs, self.X_SVs[idx])\n",
    "    self.b = np.median(b)\n",
    "\n",
    "    return\n",
    "\n",
    "  # use SVM from prediction\n",
    "  # X[N,d]: input features\n",
    "  def predict(self, X):\n",
    "    if(self.kernel != 'linear' and self.kernel != 'poly' and self.kernel != 'rbf'):\n",
    "      print(\"Error: only linear/poly/rbf kernel is supported!\")\n",
    "      return\n",
    "\n",
    "    if(self.kernel == 'linear'):\n",
    "      y = X @ self.w + self.b\n",
    "    else:\n",
    "      y = (self.y_SVs * self.alpha_SVs) @ self.Kernel(self.X_SVs, X) + self.b\n",
    "\n",
    "    return np.sign(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB1HRyMm8UYL"
   },
   "source": [
    "Displaying the accuracy with different C and gamma values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Ym2kiT3TiWl",
    "outputId": "f3265f2d-3d4c-458b-fc31-d0b4478124c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY RBF SVM (C=0.1, gamma=0.01): training accuracy=92.43%  test accuracy=93.23%\n",
      "MY RBF SVM (C=0.1, gamma=0.1): training accuracy=94.63%  test accuracy=95.04%\n",
      "MY RBF SVM (C=0.1, gamma=1): training accuracy=97.65%  test accuracy=97.79%\n",
      "MY RBF SVM (C=0.1, gamma=2): training accuracy=98.38%  test accuracy=98.28%\n",
      "MY RBF SVM (C=0.1, gamma=10): training accuracy=99.55%  test accuracy=96.66%\n",
      "MY RBF SVM (C=1, gamma=0.01): training accuracy=94.45%  test accuracy=94.99%\n",
      "MY RBF SVM (C=1, gamma=0.1): training accuracy=96.37%  test accuracy=96.71%\n",
      "MY RBF SVM (C=1, gamma=1): training accuracy=99.62%  test accuracy=98.87%\n",
      "MY RBF SVM (C=1, gamma=2): training accuracy=99.81%  test accuracy=99.36%\n",
      "MY RBF SVM (C=1, gamma=10): training accuracy=100.00%  test accuracy=98.97%\n",
      "MY RBF SVM (C=2, gamma=0.01): training accuracy=94.95%  test accuracy=95.48%\n",
      "MY RBF SVM (C=2, gamma=0.1): training accuracy=96.91%  test accuracy=97.10%\n",
      "MY RBF SVM (C=2, gamma=1): training accuracy=99.75%  test accuracy=98.97%\n",
      "MY RBF SVM (C=2, gamma=2): training accuracy=99.95%  test accuracy=99.26%\n",
      "MY RBF SVM (C=2, gamma=10): training accuracy=100.00%  test accuracy=98.97%\n",
      "MY RBF SVM (C=4, gamma=0.01): training accuracy=95.55%  test accuracy=96.07%\n",
      "MY RBF SVM (C=4, gamma=0.1): training accuracy=97.58%  test accuracy=97.50%\n",
      "MY RBF SVM (C=4, gamma=1): training accuracy=99.89%  test accuracy=99.17%\n",
      "MY RBF SVM (C=4, gamma=2): training accuracy=99.98%  test accuracy=99.56%\n",
      "MY RBF SVM (C=4, gamma=10): training accuracy=100.00%  test accuracy=98.97%\n",
      "MY RBF SVM (C=10, gamma=0.01): training accuracy=95.88%  test accuracy=96.32%\n",
      "MY RBF SVM (C=10, gamma=0.1): training accuracy=98.04%  test accuracy=97.30%\n",
      "MY RBF SVM (C=10, gamma=1): training accuracy=99.93%  test accuracy=99.12%\n",
      "MY RBF SVM (C=10, gamma=2): training accuracy=100.00%  test accuracy=99.26%\n",
      "MY RBF SVM (C=10, gamma=10): training accuracy=100.00%  test accuracy=98.97%\n"
     ]
    }
   ],
   "source": [
    "C_values = [0.1, 1, 2, 4, 10]\n",
    "gamma_values = [0.01, 0.1, 1, 2, 10]\n",
    "\n",
    "for c in C_values:\n",
    "  for g in gamma_values:\n",
    "    svm = mySVM2(max_epochs=20, lr=1.0, C=c, kernel='rbf', gamma=g, debug=0)\n",
    "    svm.fit(X_train, y_train)\n",
    "    predict_train = svm.predict(X_train)\n",
    "    train_acc = np.count_nonzero(np.equal(predict_train, y_train)) / y_train.size\n",
    "    predict_test = svm.predict(X_test)\n",
    "    test_acc = np.count_nonzero(np.equal(predict_test, y_test)) / y_test.size\n",
    "    print(f'MY RBF SVM (C={c}, gamma={g}): training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKdmOppZ8XYp"
   },
   "source": [
    "Displaying the timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmmOg7QkTieK",
    "outputId": "35795592-a302-4519-e4da-91ab11e53368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 s ± 285 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "18.8 s ± 352 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "c = 1\n",
    "gamma = 0.1\n",
    "\n",
    "# Scikit-learn RBF SVM\n",
    "rbfSVM = SVC(kernel='rbf', C=c, gamma=gamma)\n",
    "%timeit rbfSVM.fit(X_train, y_train)\n",
    "\n",
    "# PGD-based RBF SVM\n",
    "svm = mySVM2(max_epochs=10, lr=2.0, C=c, kernel='rbf', gamma=gamma)\n",
    "%timeit svm.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "E1OwmDh6Zv_b",
    "RgoTthU7aoV2",
    "Od0psVpKgX8e"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
